{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "from stanza.server import CoreNLPClient\n",
    "from empath import Empath\n",
    "import requests\n",
    "import json\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "lexicon = Empath()\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_category(self,name,seeds,model=\"fiction\",size=100,write=True):\n",
    "#   This function creates new lemmatized lexical categories\n",
    "    resp = requests.post(self.backend_url + \"/create_category\", json={\"terms\":seeds,\"size\":size,\"model\":model})\n",
    "    results = json.loads(resp.text)\n",
    "    lemma_words = list()\n",
    "    lemmatizer = WordNetLemmatizer() \n",
    "#     Lemmatize the words in response\n",
    "    for word in results:\n",
    "        lemma_words.append(lemmatizer.lemmatize(word))\n",
    "    self.cats[name] = list(set(lemma_words))\n",
    "#     Permanently store these categories\n",
    "    if write:\n",
    "        with open(self.base_dir+\"/data/user/\"+name+\".empath\",\"w\") as f:\n",
    "            f.write(\"\\t\".join([name]+results))\n",
    "\n",
    "Empath.create_lemma_category = create_category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lexicons(rb,lv,fp,ct):\n",
    "#     This function creates the specified lexical categories with the specified dictionary sizes\n",
    "    lexicon.create_lemma_category(\"religious_buildings\", [\"church\",\"mosque\", \"temple\"], model=\"fiction\", size = rb)\n",
    "    lexicon.create_lemma_category(\"loc_verbs\", [\"arrive\", \"visit\", \"travel\", \"return\"], model = \"fiction\", size= lv)\n",
    "    lexicon.create_lemma_category(\"fictional_places\", [\"place\",\"buildings\"], model =\"fiction\", size =fp)\n",
    "    lexicon.create_lemma_category(\"custom_times\", [\"once_upon_a_time\", \"next_day\",\"that_evening\"], size = ct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_lexicons(30,14,300,300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nltk_tag_to_wordnet_tag(nltk_tag):\n",
    "#     This converts nltk tag to wordent tag\n",
    "    if nltk_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif nltk_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif nltk_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif nltk_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:          \n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_and_annotate(story):\n",
    "#     Opens the story file from the specified path\n",
    "    file = open('./Panchatantra/'+story+'.txt', errors='ignore')\n",
    "    text = file.read()\n",
    "    file.close()\n",
    "#     CoreNLP Client performs processing on the input text and annotates it\n",
    "    with CoreNLPClient(annotators = ['tokenize','ssplit'],\n",
    "        memory='5G', be_quiet=True, outputFormat = 'json', max_char_length=500000, timeout=36000000) as client:\n",
    "        ann = client.annotate(text)\n",
    "#     Opens the manually annotated sentence to character map files\n",
    "    file = open(\"./Panchatantra/\"+story+'_sc.gpickle', 'rb')\n",
    "    sen_char = pickle.load(file)\n",
    "    file.close()\n",
    "    return text, ann, sen_char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_sentence(sentence):\n",
    "    #tokenize the sentence and find the POS tag for each token\n",
    "    nltk_tagged = nltk.pos_tag(nltk.word_tokenize(sentence))  \n",
    "    #tuple of (token, wordnet_tag)\n",
    "    wordnet_tagged = map(lambda x: (x[0], nltk_tag_to_wordnet_tag(x[1])), nltk_tagged)\n",
    "    lemmatized_sentence = []\n",
    "    for word, tag in wordnet_tagged:\n",
    "        if tag is None:\n",
    "            #if there is no available tag, append the token as is\n",
    "            lemmatized_sentence.append(word)\n",
    "        else:        \n",
    "            #else use the tag to lemmatize the token\n",
    "            lemmatized_sentence.append(lemmatizer.lemmatize(word, tag))\n",
    "    return \" \".join(lemmatized_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def events_by_location_and_time(text,ann):\n",
    "    \"\"\"\n",
    "    Non-hierarchy model\n",
    "    \"\"\"\n",
    "    #This function finds sum of dictionary returned by lexicon.analyze i.e., it finds the presence of location_time words.\n",
    "    def sum_of_locs_times_dict(dictionary):\n",
    "        sum_ = 0\n",
    "        for key in dictionary.keys():\n",
    "            sum_ = sum_ + dictionary[key]\n",
    "        return sum_\n",
    "    \n",
    "    \n",
    "    lexicon = Empath()   #Part of code used to bring Empath in\n",
    "    location_time = None    #The variable place will hold latest location_time word.\n",
    "    location_time_words = set()\n",
    "    location_time_by_sentence = dict() #Dictionary to store the location_time-time words in a given sentence\n",
    "    loc_num = 0 # Will be used to put location_time words as numbers in the location_time_to_number dict\n",
    "    total_sentences = 0\n",
    "    \n",
    "    #Take each sentence of the story one by one (ann.sentence returns individual sentences of the story as objects)\n",
    "    for i, sentence in enumerate(ann.sentence):\n",
    "        # Remove comma and fullstop beacuse lexicon.analyze cannot identify words if they are followd by a fullstop or comma.\n",
    "        # text[characterOffsetBegin:characterOffserEnd] is the actual sentence (as a string) of the sentence object returned\n",
    "        sentence_for_empath = text[sentence.characterOffsetBegin:sentence.characterOffsetEnd].replace(\", \",\" \").replace(\".\",\"\").replace(\"-\",\" \").replace(\"?\",\"\").replace(\"!\",\"\").replace(\":\",\" \")\n",
    "        #Lemmatize the words you encounter for better identification when being analysed by lexicon.analyze\n",
    "        #May be commented out because lexicon.create_category does not give good words when singular words are used\n",
    "        sentence_for_empath = lemmatize_sentence(sentence_for_empath) # Sentences are all lemmatized now\n",
    "        # Analyze the things\n",
    "        lexicon_location_times_dict = lexicon.analyze(sentence_for_empath,\n",
    "                                                categories=[\"religious_buildings\", \"loc_verbs\", \"fictional_places\", \"custom_times\"])\n",
    "        location_time_by_sentence[i] = list()\n",
    "        s = sum_of_locs_times_dict(lexicon_location_times_dict)\n",
    "        if s>0:\n",
    "            words = sentence_for_empath.split(\" \")\n",
    "            # Find if place is same as previous\n",
    "            for word in words:\n",
    "                # If the word is a location_time word\n",
    "                if sum_of_locs_times_dict(lexicon.analyze(word,\n",
    "                                                   categories=[\"religious_buildings\", \"loc_verbs\", \"fictional_places\", \"custom_times\"]))>0:\n",
    "                    location_time = word\n",
    "                    if i==0:\n",
    "                        location_time_words.add(location_time) #stores which words have occured previously\n",
    "                    else:\n",
    "#                         check if such a word has occured way too much previously and now refers to a new location or time\n",
    "                        if (location_time not in location_time_by_sentence[i-1] and location_time in location_time_words):\n",
    "                            location_time+=\"1\" #slightly modify the word\n",
    "                            location_time_words.add(location_time) #stores which words have occured previously\n",
    "                    location_time_by_sentence[i].append(location_time)\n",
    "        else: \n",
    "            if location_time is None:\n",
    "                location_time_by_sentence[i] = [\"UNKNOWN\"] #Unknown is the default value\n",
    "            else:\n",
    "                location_time_by_sentence[i]=location_time_by_sentence[i-1]\n",
    "        total_sentences = i\n",
    "    return location_time_by_sentence, total_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distance(cluster1, cluster2, sen_char, loc_time):\n",
    "    char_set1 = set()\n",
    "    loc_time_set1 = set()\n",
    "#     Find the first cluster's characters, location and time words and store them as sets\n",
    "    for i in cluster1:\n",
    "        if i in sen_char:\n",
    "            char_set1 = char_set1.union(set(sen_char[i]))\n",
    "        if i in loc_time:\n",
    "            loc_time_set1 = loc_time_set1.union(set(loc_time[i]))\n",
    "    char_set2 = set()\n",
    "    loc_time_set2 = set()\n",
    "#     Find the second cluster's characters, location and time words and store them as sets\n",
    "    for i in cluster2:\n",
    "        if i in sen_char:\n",
    "            char_set2 = char_set2.union(set(sen_char[i])) \n",
    "        if i in loc_time:\n",
    "            loc_time_set2 = loc_time_set2.union(set(loc_time[i]))\n",
    "#     Find the distance between character sets\n",
    "    char_dist = (len(char_set1.union(char_set2))-len(char_set1.intersection(char_set2)))/(len(char_set1.intersection(char_set2))+1)\n",
    "#     Find the length of both the clusters combined. It acts as a penalty to not enlarge large clusters\n",
    "    event_dist = (len(cluster1 + cluster2))\n",
    "#     Find the distance between location and time sets\n",
    "    loc_dist = (len(loc_time_set1.union(loc_time_set2))-len(loc_time_set1.intersection(loc_time_set2)))/(len(loc_time_set1.intersection(loc_time_set2))+1)\n",
    "#     Calculate the overall distance\n",
    "    dist = (1 + char_dist)*(1 + loc_dist)*(event_dist) \n",
    "    return dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def HAC(text, annotations, sen_char_map, no_split_points):\n",
    "#     initiate clusters\n",
    "    location_by_sentence, sentences = events_by_location_and_time(text, annotations)\n",
    "    clusters = [[i] for i in range(sentences) if i not in no_split_points]\n",
    "    iterations = [clusters.copy()]\n",
    "    event_end_points = []\n",
    "    while len(clusters)>1:\n",
    "#         calculate the distance between clusters\n",
    "        dist = np.array([distance(clusters[i], clusters[i+1], sen_char_map, location_by_sentence) for i in range(len(clusters)-1)])\n",
    "#         find the clusters with minimum distance\n",
    "        sort = np.argsort(dist) \n",
    "#         merge two clusters with minimum distance below a certain threshold\n",
    "        if dist[sort[0]]<=10:\n",
    "            index = sort[0]\n",
    "            clusters[index].extend(clusters[index+1])\n",
    "            clusters.remove(clusters[index+1])\n",
    "        else:\n",
    "            event_end_points = [max(0, cluster[0]-1) for cluster in clusters]\n",
    "            break\n",
    "#         variable to form the dendogram\n",
    "        iterations.append(clusters.copy())\n",
    "    return event_end_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the_story_of_the_merchant_son', 'the_thief_and_the_brahmins', 'the_monkey_and_the_crocodile', 'the_monkey_the_wedge', 'a_daring_plan', 'Buddha_remains_cool', 'moocha_raja', 'raman_horse_trainer', 'talkative_turtle', 'tenali_outwits_guards', 'tenali_the_detective']\n"
     ]
    }
   ],
   "source": [
    "# Opens the file containing the story names\n",
    "story_names = []\n",
    "file = open(\"./Panchatantra/Storynames.txt\")\n",
    "file_story_names = file.readlines()\n",
    "for name in file_story_names:\n",
    "    story_names.append(name.strip('\\n'))\n",
    "file.close()\n",
    "# Total sentences present in the respective stories\n",
    "total_sentences = [66,49,61,12,34,33,38,23,57,38,39]\n",
    "print(story_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-04-25 22:25:48 INFO: Writing properties to tmp file: corenlp_server-5925894f34df47e5.props\n",
      "2021-04-25 22:25:48 INFO: Starting server with command: java -Xmx5G -cp C:\\Users\\Sourav\\stanza_corenlp\\* edu.stanford.nlp.pipeline.StanfordCoreNLPServer -port 9000 -timeout 36000000 -threads 5 -maxCharLength 500000 -quiet True -serverProperties corenlp_server-5925894f34df47e5.props -annotators tokenize,ssplit -preload -outputFormat serialized\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the_story_of_the_merchant_son\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-04-25 22:25:50 INFO: Writing properties to tmp file: corenlp_server-a1b91bcde00b4d5e.props\n",
      "2021-04-25 22:25:50 INFO: Starting server with command: java -Xmx5G -cp C:\\Users\\Sourav\\stanza_corenlp\\* edu.stanford.nlp.pipeline.StanfordCoreNLPServer -port 9000 -timeout 36000000 -threads 5 -maxCharLength 500000 -quiet True -serverProperties corenlp_server-a1b91bcde00b4d5e.props -annotators tokenize,ssplit -preload -outputFormat serialized\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-04-25 22:25:51 INFO: Writing properties to tmp file: corenlp_server-8b1fe2b48b624a4a.props\n",
      "2021-04-25 22:25:51 INFO: Starting server with command: java -Xmx5G -cp C:\\Users\\Sourav\\stanza_corenlp\\* edu.stanford.nlp.pipeline.StanfordCoreNLPServer -port 9000 -timeout 36000000 -threads 5 -maxCharLength 500000 -quiet True -serverProperties corenlp_server-8b1fe2b48b624a4a.props -annotators tokenize,ssplit -preload -outputFormat serialized\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the_thief_and_the_brahmins\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-04-25 22:25:53 INFO: Writing properties to tmp file: corenlp_server-720fe11f9032443b.props\n",
      "2021-04-25 22:25:53 INFO: Starting server with command: java -Xmx5G -cp C:\\Users\\Sourav\\stanza_corenlp\\* edu.stanford.nlp.pipeline.StanfordCoreNLPServer -port 9000 -timeout 36000000 -threads 5 -maxCharLength 500000 -quiet True -serverProperties corenlp_server-720fe11f9032443b.props -annotators tokenize,ssplit -preload -outputFormat serialized\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the_monkey_and_the_crocodile\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-04-25 22:25:54 INFO: Writing properties to tmp file: corenlp_server-c7aaa448505f48be.props\n",
      "2021-04-25 22:25:54 INFO: Starting server with command: java -Xmx5G -cp C:\\Users\\Sourav\\stanza_corenlp\\* edu.stanford.nlp.pipeline.StanfordCoreNLPServer -port 9000 -timeout 36000000 -threads 5 -maxCharLength 500000 -quiet True -serverProperties corenlp_server-c7aaa448505f48be.props -annotators tokenize,ssplit -preload -outputFormat serialized\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the_monkey_the_wedge\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-04-25 22:25:55 INFO: Writing properties to tmp file: corenlp_server-261307cc6aa24fc7.props\n",
      "2021-04-25 22:25:55 INFO: Starting server with command: java -Xmx5G -cp C:\\Users\\Sourav\\stanza_corenlp\\* edu.stanford.nlp.pipeline.StanfordCoreNLPServer -port 9000 -timeout 36000000 -threads 5 -maxCharLength 500000 -quiet True -serverProperties corenlp_server-261307cc6aa24fc7.props -annotators tokenize,ssplit -preload -outputFormat serialized\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_daring_plan\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-04-25 22:25:57 INFO: Writing properties to tmp file: corenlp_server-e99cdd2a26504f75.props\n",
      "2021-04-25 22:25:57 INFO: Starting server with command: java -Xmx5G -cp C:\\Users\\Sourav\\stanza_corenlp\\* edu.stanford.nlp.pipeline.StanfordCoreNLPServer -port 9000 -timeout 36000000 -threads 5 -maxCharLength 500000 -quiet True -serverProperties corenlp_server-e99cdd2a26504f75.props -annotators tokenize,ssplit -preload -outputFormat serialized\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Buddha_remains_cool\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-04-25 22:25:58 INFO: Writing properties to tmp file: corenlp_server-a26e82b48b3d4817.props\n",
      "2021-04-25 22:25:58 INFO: Starting server with command: java -Xmx5G -cp C:\\Users\\Sourav\\stanza_corenlp\\* edu.stanford.nlp.pipeline.StanfordCoreNLPServer -port 9000 -timeout 36000000 -threads 5 -maxCharLength 500000 -quiet True -serverProperties corenlp_server-a26e82b48b3d4817.props -annotators tokenize,ssplit -preload -outputFormat serialized\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "moocha_raja\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-04-25 22:25:59 INFO: Writing properties to tmp file: corenlp_server-da594a9cf95e4f53.props\n",
      "2021-04-25 22:25:59 INFO: Starting server with command: java -Xmx5G -cp C:\\Users\\Sourav\\stanza_corenlp\\* edu.stanford.nlp.pipeline.StanfordCoreNLPServer -port 9000 -timeout 36000000 -threads 5 -maxCharLength 500000 -quiet True -serverProperties corenlp_server-da594a9cf95e4f53.props -annotators tokenize,ssplit -preload -outputFormat serialized\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "raman_horse_trainer\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-04-25 22:26:01 INFO: Writing properties to tmp file: corenlp_server-1cec778b36794c78.props\n",
      "2021-04-25 22:26:01 INFO: Starting server with command: java -Xmx5G -cp C:\\Users\\Sourav\\stanza_corenlp\\* edu.stanford.nlp.pipeline.StanfordCoreNLPServer -port 9000 -timeout 36000000 -threads 5 -maxCharLength 500000 -quiet True -serverProperties corenlp_server-1cec778b36794c78.props -annotators tokenize,ssplit -preload -outputFormat serialized\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "talkative_turtle\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-04-25 22:26:02 INFO: Writing properties to tmp file: corenlp_server-2b26028693ef485a.props\n",
      "2021-04-25 22:26:02 INFO: Starting server with command: java -Xmx5G -cp C:\\Users\\Sourav\\stanza_corenlp\\* edu.stanford.nlp.pipeline.StanfordCoreNLPServer -port 9000 -timeout 36000000 -threads 5 -maxCharLength 500000 -quiet True -serverProperties corenlp_server-2b26028693ef485a.props -annotators tokenize,ssplit -preload -outputFormat serialized\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tenali_outwits_guards\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "tenali_the_detective\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "scenes = []\n",
    "# no splitting_points stores the sentences which have dialogues in them and therefore scene splitting cannot be performed in those sentences\n",
    "no_splitting_points = dict()\n",
    "for sn in range(len(story_names)):\n",
    "    text , annotated_story, sen_char_map = open_and_annotate(story_names[sn])\n",
    "    no_splitting_here = []\n",
    "    flag=0  #stores the start and end of a dialogue\n",
    "    for i, sentence in enumerate(annotated_story.sentence):\n",
    "        sentence = text[sentence.characterOffsetBegin:sentence.characterOffsetEnd].strip(\"\\n\").replace(\", \",\" \").replace(\".\",\"\").replace(\"-\",\" \").replace(\"?\",\"\").replace(\"!\",\"\").replace(\":\",\" \").replace(\"\\n\\n\",\"\").replace(\"\\n \\n\", '')\n",
    "        no_of_quotes = sentence.count('\"') #the number of double quotes in the sentence\n",
    "        if no_of_quotes%2 != 0:        \n",
    "            flag = (flag + 1)%2\n",
    "        if flag != 0 or (no_of_quotes!=0):\n",
    "            no_splitting_here.append(i)\n",
    "    print(story_names[sn])\n",
    "#     append the scenes after Hierarchical Agglomerative Clustering\n",
    "    scenes.append(HAC(text, annotated_story, sen_char_map, no_splitting_here))\n",
    "    print(\"\\n\\n\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the_story_of_the_merchant_son \t [6, 7, 10, 19, 27, 33, 39, 44, 50, 51, 62, 64, 65] \n",
      "\n",
      "the_thief_and_the_brahmins \t [1, 7, 12, 22, 28, 43, 48] \n",
      "\n",
      "the_monkey_and_the_crocodile \t [9, 15, 25, 50, 60] \n",
      "\n",
      "the_monkey_the_wedge \t [1, 3, 6, 11] \n",
      "\n",
      "a_daring_plan \t [8, 11, 18, 23, 33] \n",
      "\n",
      "Buddha_remains_cool \t [2, 4, 15, 25, 32] \n",
      "\n",
      "moocha_raja \t [1, 2, 8, 16, 22, 28, 30, 37] \n",
      "\n",
      "raman_horse_trainer \t [1, 3, 7, 10, 11, 16, 18, 22] \n",
      "\n",
      "talkative_turtle \t [4, 6, 26, 28, 31, 32, 37, 46, 53, 56] \n",
      "\n",
      "tenali_outwits_guards \t [5, 9, 21, 37] \n",
      "\n",
      "tenali_the_detective \t [38] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Sorts the event end points in ascending order\n",
    "scenes = [sorted(list(set(eve))) for eve in scenes]\n",
    "for j, eve in enumerate(scenes):\n",
    "    if 0 in eve:\n",
    "        eve.remove(0)\n",
    "    last_index = total_sentences[j]-1\n",
    "    if last_index not in eve:\n",
    "        eve.append(last_index)\n",
    "for sn in range(len(story_names)):\n",
    "    print(story_names[sn],\"\\t\",scenes[sn],\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# True scene end points based on manual annotation\n",
    "true_scenes = [[7,9,17,26,31,38,41,45,49,64,65], [1,6,17,22,27,48], [2,9,11,13,24,33,48,58,60], [1,5,6,11], \n",
    "               [8,12,18,22,30,33], [2,6,15,28,32], [1,10,13,15,17,22,36,37], [3,5,10,18,22], [3,5,6,25,32,44,56], [1,3,7,12,35,37], \n",
    "              [34,38]]\n",
    "scenes_generated = scenes\n",
    "sm1 = 0\n",
    "sm2 = 0\n",
    "for i in range(len(true_scenes)):\n",
    "    m1,m2 = metrics.IoU(true_scenes[i], scenes_generated[i], total_sentences[i])\n",
    "    sm1+=m1\n",
    "    sm2+=m2\n",
    "# Average value of metrics\n",
    "avg_Jaccard = sm1/11\n",
    "avg_Penalty = sm2/11\n",
    "avg_F1_score = 2/(1/avg_Jaccard+1/(1-avg_Penalty))\n",
    "print(avg_Jaccard, avg_Penalty, avg_F1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
